import os
import tempfile

os.environ["CHROMA_DISABLE_TELEMETRY"] = "True"

import streamlit as st
import torch
from PyPDF2 import PdfReader
# â†“ dÃ¹ng loader cÃ³ sáºµn trong langchain core
from langchain.document_loaders import PyPDFLoader  
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline,
)
from langchain import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” State init â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "llm" not in st.session_state:
    st.session_state.llm = None

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Load embeddings â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Load LLM â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
@st.cache_resource
def load_llm():
    # NF4 4-bit quant config
    nf4 = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_dtype=False,
    )
    MODEL = "lmsys/vicuna-7b-v1.5"
    model = AutoModelForCausalLM.from_pretrained(
        MODEL,
        quantization_config=nf4,
        low_cpu_mem_usage=True,
        device_map="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    pipe = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        pad_token_id=tokenizer.eos_token_id,
        device_map="auto",
    )
    return HuggingFacePipeline(pipeline=pipe)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Xá»­ lÃ½ PDF â†’ RAG chain â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def process_pdf(uploaded_file):
    # 1) LÆ°u táº¡m PDF
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.getvalue())
        pdf_path = tmp.name

    # 2) Load & split vÄƒn báº£n
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = splitter.split_documents(docs)

    # 3) Build FAISS index
    embeddings = st.session_state.embeddings
    vector_db = FAISS.from_documents(chunks, embeddings)

    # 4) Chuáº©n bá»‹ prompt template
    prompt_template = """Báº¡n lÃ  trá»£ lÃ½ thÃ´ng minh. DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n context Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« PDF:
{context}

HÃ£y tráº£ lá»i cÃ¢u há»i sau báº±ng tiáº¿ng Viá»‡t, náº¿u khÃ´ng cÃ³ trong context thÃ¬ xin lá»—i vÃ  nÃ³i báº¡n khÃ´ng biáº¿t:
Question: {question}
Answer:"""
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    # 5) Táº¡o má»™t function Ä‘Æ¡n giáº£n cho RAG
    def rag_chain(question: str) -> str:
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})
        docs = retriever.get_relevant_documents(question)
        context = "\n\n".join([d.page_content for d in docs])
        text_in = prompt.format(context=context, question=question)
        return st.session_state.llm(text_in)

    # 6) Cleanup
    os.unlink(pdf_path)
    return rag_chain, len(chunks)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Giao diá»‡n â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
st.set_page_config(page_title="PDF RAG Assistant", layout="wide")
st.title("ğŸ“ PDF RAG Assistant (Tiáº¿ng Viá»‡t)")
st.markdown("""
á»¨ng dá»¥ng RAG giÃºp báº¡n há»i Ä‘Ã¡p trá»±c tiáº¿p trong PDF, chá»‰ vá»›i vÃ i bÆ°á»›c Ä‘Æ¡n giáº£n:

1. **Upload** file PDF.
2. **Process** Ä‘á»ƒ tÃ¡ch vÃ  index.
3. **Ask** cÃ¢u há»i, nháº­n tráº£ lá»i ngay!
""")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Load model láº§n Ä‘áº§u â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if not st.session_state.model_loaded:
    with st.spinner("Äang táº£i model vÃ  embeddingsâ€¦"):
        st.session_state.embeddings = load_embeddings()
        st.session_state.llm = load_llm()
        st.session_state.model_loaded = True
    st.success("Model vÃ  embeddings Ä‘Ã£ sáºµn sÃ ng!")
    st.experimental_rerun()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Upload & Process â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
uploaded = st.file_uploader("Chá»n file PDF", type=["pdf"])
if uploaded and st.button("Process PDF"):
    with st.spinner("Äang xá»­ lÃ½ PDFâ€¦"):
        st.session_state.rag_chain, n_chunks = process_pdf(uploaded)
    st.success(f"PDF Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ vÃ  chia thÃ nh {n_chunks} chunks.")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Há»i Ä‘Ã¡p â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if st.session_state.rag_chain:
    q = st.text_input("Nháº­p cÃ¢u há»i cá»§a báº¡n:")
    if q:
        with st.spinner("Äang sinh cÃ¢u tráº£ lá»iâ€¦"):
            resp = st.session_state.rag_chain(q)
        st.markdown("**Answer:**")
        st.write(resp)
